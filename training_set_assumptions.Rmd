---
title: "Implicit training set assumptions - an example from an anticancer drug classifier"
author: "Kreshnik Xhangolli"
date: "April 20, 2017"
output: html_document
---

## Abstract

The usual approach to supervised learning is to divide *available data* into training and testing set, train machine learning algorithms on the former, test their perfomance on the latter, choose the algorithm with best predicting perfomance on training set and finally apply the algorithm to predict outcomes on *new* records. We discuss that in the case when the population represented by *available data* is a proper subset of the population represented by *new* records, applying an algorithm trained on *available data* onto *new* records introduces implicit assumptions that need to be considered. To demonstrate our point we analyze the anticancer drug classifier developed by Heinemann *et al*. (2016)[^1] and emphasize the implicit assumptions made if such a classifier is applied to *new* drugs.

[^1]: Heinemann et al. 2016. *"Reflection of successful anticancer drug development processes in the literature"*. Drug Discovery Today. Vol 21. Issue 11. Pg. 1740-1744. DOI: http://dx.doi.org/10.1016/j.drudis.2016.07.008

## Introduction

Drug developemnt by pharmaceuticals is usually a structured process that undergoes stages of approval and monitoring from government authorities. A schematic presentation is shown in the [Figure 1](http://www.sciencedirect.com/science/article/pii/S1359644616302586) of Heinemann *et al* (2016). The authors describe the process with the first step being the proposal of a molecule as *target* by one or more published articles. These first appearances cause more research and interest on the topic which at some point attracts the attention of pharmaceuticals R&D. The pharmaceuticals drug development process starts with the TI/V phase (target identification/validation) and usually lags the apperance of first publications. Then the drug is developed through the sequential cases of IL (identification of lead compound), LO (lead optimization), PC (preclinical tests), P1, P2 and P3 (phase 1,2 and 3 clinical trials) and AL (approval and launch).

Heinemann *et al* (2016) assume that "*[...] there are differences in how researchers collectively publish findings that ultimately lead to approved drugs and those that fail*". Upon this assumption they build classifiers who considers as features publication characteristics such as article counts, publication history of authors, research commitment, industry etc. The authors describe in detail the features used in the Supplementary Materials. The dataset was created by selecting from FDA two groups of anitcancer drugs, approved and failed at phase 2/3 of clinical trials. For each of the selected drugs a target-indication pair was identified; indication referrs to symptoms/cancer such as multiple myeloma, prostate cancer, Chronic myelogenous leukemia etc. and target referrs to targeted molecule such as 0S Proteasome, ALK, Androgene receptor etc and  ("*Table S1.docx*" of Supplementary Materials). The T-I pairs are needed to retrieve publications since it is usually the symptoms and the targeted molecule which is specified in published articles. The T-I pairs for approved drugs are labeled as Class 1 while those for failed drugs as Class 2. For comparisons, Heinemann *et al* (2016) identify also a Class 3 of T-I pairs which do not correspond to any drugs that has been approved for clinical trial. In this way, each record, which belongs to either Class 1,2 or 3 is indexed by a T-I pair. The T-I pair is later used to measure all the features applied to the machine learning algorithm.

Heinemann *et al* (2016) make two comparisons:
 
 * Class 1 (C1) vs. Class 2 (C2) ("Supporting text and figures", pg. 3, par. 2) and 
 
 * Comparison of class 3 against classes 1 and 2 ("Supporting text and figures", pg. 3, par. 3).
 
Classification of C1 vs. C2 was done with ML ("Supporting text and figures", pg. 3, par. 4) by using as training set the T-I pairs identified in "DatasetS1" and "DatasetS2" together with 9 feature classes. In total 10 classifiers were trained, representing the years before decision time. Decision time was considered the reference point and time frame moved backward. The ML algorithms' performance was tested on a new set of drugs whose outcome for P2/3 or AL was released recently. Instead,	comparison of C3 vs. C1 and C2 was done with Mann-Whitney-Wilcoxon two tailed test with significance set at 5%. In this comparison the reference time was considered the year of the first publication and the time frame moved forward.


## Implicit Assumptions

In this section we clarify the implicit assumption necessary when predicting, using the algorithm, the success or approval of a new drug. We also give examples when the prediction might be misspecified. We want to emphasize that we are not critisizing the approach per se, but rather stressing the importance of the implicit assumptions. If these implicit assumptions hold then there should be no objection upon employing such prediction algorithm.

The training dataset and the algorithm were set in such a way that the decision time on both training and test set was known apriori and all features were constructed using the decision time as the reference point. In other words the algorithm knows or assumes that the drug has reached at least P2/3 and a decision has been made. This can also be interpreted that the algorithm predicts only conditional on the fact that a decision has been taken at P2/3 or AL (i.e when decision time (DT) is known or assumed). If this condition is met, i.e the drug has reached at least P2/3 and decision has been made, then the test shows that the algorithm does a good job in predicting even several years in advance approval or rejection of such drugs. 

Therefore when applying the algorithm we need to know beforehand that the drug has reached P2/3 and a decision has been made. This was the case with the testing set upon which classification algorithms were tested. Another approach would be to assume that the drug would make it to P2/3 and a decision will be made at some time. The implication of such assumption depend on how plausible such assumption is. Below we present 3 hypothetical scenarios and consider the implications of such assumption. 

*	*Example 1:* A drug is in P2/3 but a decision has not been taken yet. In this case we need to assume only that a decision will be taken sometime in the future by setting decision time to next year, next 3 years or next 5 years. We could also try to set the decision time to next 7 years but the predictive power of the algorithm drops at this time frame. According to the results from the test, running the algorithm in this case and under decision times of next 1,3 or 5 years should produce good predictions. 
It is important to note that the implicit assumption here is that once a drug is in P2/3 a decision will be taken in all cases. This looks like a fair assumption since when a drug is in clinical trials a decision will be taken in any case.

*	*Example 2:* A drug is in PC or P1. In this case we are making two assumptions (i) the drug will reach P2/P3 and (ii) a decision will be taken in the future. We run the prediction by setting decision time to next 1,3,5 or 7 years. The severity of the first assumption depends on how frequent is for a drug to get from PC or P1 stage to P2/P3 and that a decision will be made. The concerns of the validity of assumption (i) are stronger, since a drug may fail stages PC or P1 and does not proceed to P2/3. Since the algorithm was not trained on such cases, it cannot differentiate if the considered case is among them. So differently from example 1, in this example we need also to consider the specificity of the drug research development, i.e. the development happens in stages and some drugs may fail earlier stages than P2/3. This information is not captured by considering only the time dimension and training the algorithm only on classes C1 and C2. The algorithm will approximate the prediction to whichever class the considered class is closer, despite the fact that the drug might fail in stage P1.

*	*Example 3:* Assume that for some reason we know beforehand the true process, which is binary logistic and all the past dependencies are captured by features in the decision time year ($t(0)$) and a year before($t(-1)$). This means that all the information from before two years of decision time $(t(-2), t(-3),.)$ does affect approval of rejection of drug, therefore we are not interested in those years. In addition, for simplicity we will assume that only two features play a role, $f^C_{TIy}$ and $f^N_{TIy}$; respectively article counts and normalized article counts. Such model would be represented mathematically as: $$ log(\frac{\pi}{1-\pi}) = b_0 + b_1 f^C_{TI,t(0)} + b_2 f^C_{TI,t(-1)}  + b_3 f^N_{TI,t(0)} + b_4 f^N_{TI,t(-1)} $$ where $\pi=P(approval|f^C_{TI,t(0)},f^C_{TI,t(-1)},f^N_{TI,t(0)},f^N_{TI,t(-1)})$. We use the training set to estimate coefficients $\hat{b_0},\hat{b_1},\hat{b_2},\hat{b_3},\hat{b_4}$. Prediction of success of a *new* drug will be done on estimated value $\hat{\pi}$ and the decision cutoff we have agreed upon (usually 0.5). If we tested these estimates with drugs/records from the training set used by Heinemann *et al* (2016), where decision time was known before hand (either 2014 or 2015), we would get good prediction results; the implicit assumption is met and the algorithm predict well at $t(-1)$ lag. 
Now assume that we test a drug such as in Example 1, which is in P2/P3 but a decision has not been taken yet. Since we know that the decision process depends only up to $t(-1)$, we can only assume that the decision will be made the current year and run the prediction. If the decision on the drug predicted will indeed be made the current year, then we are using the right values of $f^C_{TI,t(0)},f^C_{TI,t(-1)},f^N_{TI,t(0)},f^N_{TI,t(-1)}$ and the prediction has good chances of being correct. Instead if the decision on the drug predicted will be made in the coming year, then we are using values $f^C_{TI,t(-1)},f^C_{TI,t(-2)},f^N_{TI,t(-1)},f^N_{TI,t(-2)}$ and we know from the true model that $f^C_{TI,t(-2)},f^N_{TI,t(-2)}$ Therefore our estimation of the probability of approval is misspecified.

To summarize, running a prediction on a new drug on which a decision has not been taken (therefore there is no decision time as a reference), will make implicit assumption for which we need to be aware of. If the assumptions made are theoretically reasonable, then we have more confidence in the prediction made by the algorithm. Otherwise more caution should be taken when considering the prediction result.

## Suggestions
### Introduce stage-dependent information
One approach to tackling complex analysis is to do a thought experiments on the ideal dataset and then check the differences between the dataset at hand with the ideal one. We use the same approach to propose next steps for the analysis. We clarify that this exercise is done with our current level of understanding of the underlying drug development processes. 

The ideal dataset would contain each T-I pair and the stage in which it stops. Here we assume that research on a new drug stops for some reason at one of the stages of the pharmaceutical development (TI/V, IL, LO, PC, P1/2/3, AL). For example the successful drugs would stop at stage AL, while the rejected drugs would stop at any of the stages (TI/V, IL, LO, PC, P1/2/3, AL). In the ideal dataset we would be able to tell at which end-stage category a drug and T-I pair belong. This could happen if outcomes similar to *"Terminated, lack of efficacy"*, *"Terminated, Safety/adverse effects"* or *"Completed, negative outcome/primary endpoint not met"* used in P2/3 stage, would be also available at TI/V, IL, LO, PC, P1 stages.
Using the end stage of each drug and T-I pair we could build a training set containing drugs that have reached their end-stage. This would be just an extension of the C1/C2 training set used currently. We then build predictors for each stage and train the algorithms for k-level classification depending the specific stage we are considering. For example, for processes that are on PC stage we train the algorithm for a 4-level classification. In this case the algorithm should be able to differentiate between processes that have end stages PC, P1, P2/3 or AL by analyzing all the information prior to PC stage. If the algorithm cannot differentiate late stages such as P2/3 or AL when considering data from PC stage and before, we could also opt for dichotomous classification of the kind that goes to the next stage or not.
This approach, which we label *ideal approach*, differs from the one used in Heinemann *et al* (2016), which we label *article approach*. In the *ideal approach* knowing of stages sequence removes the time uncertainty, since we know which stage comes next. This does not mean that we do not consider time in the *ideal approach*; one of the features could be easily the number of years a drug has been in a certain stage. In contrast to *article approach*, the *ideal approach* considers the specificity of the drug research development, i.e. the development happens in stages and some drugs may fail at each stage. In addition we could calculate features depending on stage level rather than time. Another advantage of *ideal approach* is that we have by definition balanced datasets. In *article approach*, the time frame for each drug changed and it required manipulations such as padding with 0s. On some cases these artificial changes may introduce biases. Instead in *ideal approach* we would consider the same number of stages. We can categorize the period from the first publication to TI/V as pre target identification/validation (P-TI/V).

### Identify a class of drugs that have failed prior to P2/3

If the ideal dataset is not attainable, then it is beneficial to compare the differences of the database at hand with the ideal one. Our understanding of the dataset used is that it contains features of the development of the research from the first year of publication until present for drugs with end-stage AL or P2/3. We need to consider the possibility of enriching the current dataset with (i) information on stages and (ii) identification of drugs whose end-stage is different from P2/3 and AL.
If information on stages is not available, then we can attempt to identify drugs whose end-stage is different from P2/3 and AL and add those to the article approach. We will still have the time uncertainty, but we will account somehow for the fact that not all drugs reach P2/3 stage. The algorithm will have a group of targeted drugs (not a perfect one albeit) that have failed before P2/3. In this way the algorithm can learn how to differentiate between this class and C1, C2 classes. 
If both suggestions are not feasible, then training the algorithm only on drugs that have reached either P2/3 or AL might be a version of selection bias problem (as it is referred in Econometrics). In statistical inference this problem is solved by first calculating the probability that a drug will reach P2/3 stage and that a decision will be taken in X years and then adding some function of this probability (inverse Mills ratio) with the dependent variables (equivalent of features). Reviewing literature on how to adapt this approach to machine learning or how machine learning tackles selection bias problem can be considered.
